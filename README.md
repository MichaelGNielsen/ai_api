# AI API Calling Examples (Docker & Ollama)

Dette projekt demonstrerer to m√•der at integrere AI i Python via Docker:

1. **Lokal AI:** K√∏rer p√• din egen hardware (f.eks. NUC eller Raspberry Pi 5) via Ollama.
2. **Cloud AI:** K√∏rer via Googles servere (Gemini API).

| Script | Type | AI Model | Krav |
| :--- | :--- | :--- | :--- |
| **`ai_call_http.py`** | **LOKAL** | `gemma3:4b` | Ollama-container skal k√∏re lokalt. Ingen API-n√∏gle kr√¶ves. |
| **`ai_test.py`** | **CLOUD** | `gemini-2.5-flash` | Internetforbindelse + API-n√∏gle i `.env` filen. |

---

## üöÄ Hurtig Genstart & Model-skift (Gemma 3)

Hvis din Ollama-container driller eller allerede findes, kan du bruge disse kommandoer:

**1. Genstart eller opret containeren:**
```bash
# Fjern eksisterende (hvis den er i konflikt) og k√∏r forfra
docker rm -f ollama-server
docker run -d --name ollama-server -p 11434:11434 -v ollama_data:/root/.ollama --restart always ollama/ollama
```

**2. Skift/K√∏r Gemma 3 modellen:**
```bash
docker exec -it ollama-server ollama run gemma3:4b
```

---

## 1. Ops√¶tning af Lokal AI (Ollama)

For at k√∏re den lokale AI (Gemma 3) skal du f√∏rst have "AI-motoren" (Ollama) til at k√∏re p√• din maskine. **Dette virker p√• b√•de x86 (NUC) og ARM64 (Raspberry Pi 5).**

### Start Ollama Server

K√∏r denne kommando for at starte Ollama som en baggrundsservice i Docker:

```bash
docker run -d \
  --name ollama-server \
  -p 11434:11434 \
  -v ollama_data:/root/.ollama \
  --restart always \
  ollama/ollama
```

### Download og forbered Gemma 3

F√∏rste gang du k√∏rer dette, vil den downloade modellen (det kan tage lidt tid p√• en RPi5).

```bash
docker exec -it ollama-server ollama run gemma3:4b
```

*(N√•r du f√•r en prompt frem, er modellen klar. Tryk `Ctrl+D` for at afslutte prompten - serveren k√∏rer videre i baggrunden).*

### Test at serveren k√∏rer

Du kan tjekke om serveren lytter ved at k√∏re:

```bash
curl http://localhost:11434
# Forventet svar: Ollama is running
```

---

## 2. K√∏r Python Scripts via Docker Compose

Vi bruger `docker compose` til at k√∏re vores Python-scripts. Dette sikrer, at de k√∏rer i et isoleret milj√∏ med de rigtige pakker (fra `requirements.txt`), og at de kan finde din lokale Ollama-server.

### Forberedelse

S√∏rg for, at du har en `.env` fil i projektets rodmappe. Den skal se s√•dan ud:

```env
# API-n√∏gle til ai_test.py (Cloud)
VITE_API_KEY=din_google_gemini_api_n√∏gle_her

# Netv√¶rks-routing s√• Docker kan finde din host-maskine (Ollama)
LLM_HOST=host.docker.internal
```

### Byg milj√∏et (hvis du har √¶ndret i koden)

```bash
docker compose build
```

### K√∏r Lokal AI Script (`ai_call_http.py`)

Dette script kontakter din lokale Ollama-server og beder om en tekst om Romerrigets fald. Det bruger et forl√¶nget timeout, s√• f.eks. en Raspberry Pi har tid til at "t√¶nke".

```bash
docker compose run --rm ai-app python ai_call_http.py
```

### K√∏r Cloud AI Script (`ai_test.py`)

Dette script kontakter Googles servere.

```bash
docker compose run --rm ai-app python ai_test.py
```

---

## 3. Nyttige Docker Kommandoer

**Administrer Ollama Modeller:**

```bash
docker exec -it ollama-server ollama list       # Se installerede modeller
docker exec -it ollama-server ollama rm gemma3  # Slet en model for at frig√∏re plads
```

**Administrer Ollama Container:**

```bash
docker logs -f ollama-server  # Se hvad AI-serveren laver (god til fejlfinding)
docker stop ollama-server     # Stop AI-serveren midlertidigt
docker start ollama-server    # Start den igen
```

**Tr√¶d ind i Python-containeren:**
Hvis du vil snuse rundt inde i det milj√∏, hvor Python k√∏rer:

```bash
docker compose run --rm ai-app /bin/bash
```

---

## üõ†Ô∏è Hardware-specifik Dokumentation

Her kan du finde (og opdatere) noter om, hvordan koden k√∏rer p√• forskellige platforme:

*   [**WSL2 (Windows 11)**](WSL_RUN.md) - Noter om Docker-netv√¶rk og ydeevne.
*   [**NUC (Intel)**](NUC_RUN.md) - K√∏rselsstatistik for x86 hardware.
*   [**Raspberry Pi 5**](RPI5_RUN.md) - Noter om ARM64 ydeevne og timeout-indstillinger.
